# 线性回归简要理论
***回归***是能为一个或多个自变量与因变量之间关系建模的一类方法。多与*预测*任务有关。
## 线性模型
线性假设是指目标可以表示为特征的加权和，如： `$ \mathrm{y} = w_{\mathrm{1}} \cdot x_{\mathrm{1}} + w_{\mathrm{2}} \cdot x_{\mathrm{2}} + b. $`  
其中，$ w_{\mathrm{1}} $ 和 $ w_{\mathrm{2}} $ 称为权重，权重决定了每个特征对预测值的影响。 $ b $ 称为偏置。  
线性模型的预测目标是寻找模型的权重$ w $ 和$ b $ ,使得模型的预测值大题符合数据集中的真实目标数据。由于机器学习中的数据一般是高维数据，所以权重$ w $ 和 $ b $是很多的，根据上式，我们可以得到多个类似的方程式，构成方程组，然后转化成线性代数的矩阵表示。  

下面展示一下这个推导过程：  
- 首先，对一个线性方程有：$ \hat{y} = w_1  x_1 + ... + w_d  x_d + b. $ ，$ \hat{y} $表示线性模型的预测值，$ y $表示真实值。  
- 然后，将**单个样本的所有特征**放到向量 $ \mathbf{x} \in \mathbb{R}^d $ 中($ x = [x_1,x_2,...,x_d]^\top $)， 并将**单个样本的所有权重**放到向量 $ \mathbf{w} \in \mathbb{R}^d $ 中($ w = [w_1,w_2,...,w_d]^\top $ )， 我们可以用点积形式来简洁地表达模型: $\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$  
- 由上，我们知道x是一个列向量，表示一个样本数据的不同特征是列排列的。但是我们习惯上，希望一个样本数据的不同特征是行排列的，然后每一行都是一个样本。这样的化，需要对上式的$ \mathbf{w}^\top \mathbf{x} $ 转置一下。于是，我们用矩阵$ \mathbf{X} \in \mathbb{R}^{n \times d} $ (X.shape=[n, d], w.shape=[d, 1])表示我们引用了整个数据集的n个样本。预测值 $ \hat{\mathbf{y}} \in \mathbb{R}^n $ ($ \hat{\mathbf{y}}$.shape=[n, 1]) 可以通过矩阵-向量乘法表示为：$ {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b $  
这个过程中的求和将使用广播机制。 给定训练数据特征 X 和对应的已知标签 y ， 线性回归的目标是找到一组权重向量 w 和偏置 b ： 当给定从 X 的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。  

虽然我们相信给定 x 预测 y 的最佳模型会是线性的， 但我们很难找到一个有 n 个样本的真实数据集，其中对于所有的 $1 \leq i \leq n$ ， y(i) 完全等于 $\mathbf{w}^\top \mathbf{x}^{(i)}+b$ 。 无论我们使用什么手段来观察特征 X 和标签 y ， 都可能会出现少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。  
在开始寻找最好的模型参数（model parameters） w 和 b 之前， 我们还需要两个东西： （1）一种模型质量的度量方式,即损失函数； （2）一种能够更新模型以提高模型预测质量的方法，即优化算法。

## 损失函数
在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 **损失函数（loss function）能够量化目标的实际值与预测值之间的差距。** 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。 当样本 i 的预测值为 $\hat{y}^{(i)}$ ，其相应的真实标签为 $y^{(i)}$ 时， 平方误差可以定义为以下公式：  $l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$  
常数 $\frac{1}{2}$ 不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。 由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。  
《动手学深度学习》一书中，通过举例了一个一维的回归问题展示了损失函数问题。书中配图如下：
![](http://zh-v2.d2l.ai/_images/fit-linreg.svg)  
由于平方误差函数中的二次方项， 估计值 $\hat{y}^{(i)}$ 和观测值 $y^{(i)}$ 之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集 n 个样本上的损失均值（也等价于求和）。则整个训练集的损失函数为：$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$  
在训练模型时，我们希望寻找一组参数$(\mathbf{w}^*, b^*)$， 这组参数能最小化在所有训练样本上的总损失。如下式：$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$

## 解析解（显式解）
模型的解可以用一个公式简单的表达出来，这个解就是 ***解析解（显式解）***。线性回归的解析解求解办法是：
- 将偏差加入权重 $X \gets [X, 1]$ $w\gets [w, b]^\top$
- $ 损失函数 L(\mathbf{X},\mathbf{y}, \mathbf{w},) = \frac{1}{2n} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 $ 对$ \mathbf{w} $求偏导得，$ \frac{\partial L(\mathbf{X},\mathbf{y}, \mathbf{w})}{\partial \mathbf(w)}= \frac{1}{n} (\mathbf{y}-\mathbf{X}\mathbf{w})^\top \mathbf{X}  $  
- 令损失函数$ \frac{\partial L(\mathbf{X},\mathbf{y}, \mathbf{w})}{\partial \mathbf(w)}=0 \Leftrightarrow \frac{1}{n} (\mathbf{y}-\mathbf{X}\mathbf{w})^\top \mathbf{X}=0 \Leftrightarrow \mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}. $  

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

## 随机梯度下降（优化算法）
即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。  
### 梯度下降算法
神经网络中常用**梯度下降算法**来优化深度学习模型。它通过不断的在损失函数递减的方向上更新参数来降低误差。  
**梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。** 。可以如下简要的理解梯度下降算法：  
- 挑选一个权重初始值$w_0$
- 重复迭代参数t=1,2,3...,则有公式$\mathbf{w}_t = \mathbf{w}_{t-1}-\eta \frac{\partial L}{\partial \mathbf{w}_{t-1}}$, 其中，$\eta$是学习率。

### 小批量随机梯度下降算法
但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做**小批量随机梯度下降（minibatch stochastic gradient descent）**。在每次迭代中，我们首先随机抽样一个小批量 $\mathcal{B}$ ， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数 $\eta$ ，并从当前参数的值中减掉。  
我们用下面的数学公式来表示这一更新过程（ $\partial$ 表示偏导数）：  
- $l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$
- $(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$  

总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:
$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}
$   
其中，公式 中的 $\mathbf{w}$ 和 $\mathbf{x}$ 都是向量。$|\mathcal{B}|$ 表示每个小批量中的样本数，这也称为批量大小（batch size）。  $\eta$ 表示学习率（learning rate）。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为**超参数（hyperparameter）**。 **调参（hyperparameter tuning）**是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的*验证数据集（validation dataset）*上评估得到的。  
在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为 $\hat{\mathbf{w}}, \hat{b}$ 。 但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。  

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化（generalization）**。

## 总结
- 线性回归是对n维输入的加权，再加偏置
- 使用平方损失来衡量预测值和真实值的差异
- 线性回归有显式解
- 线性回归可以看做是单层神经网络
- 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降是深度学习默认的求解算法
- 两个重要的超参数是批量大小和学习率

# 卷积神经网络QA集合

## 卷积层QA

- 100个神经元的MLP和100维的全连接层有什么区别？
  
  **答：** 没有什么区别。100个神经元的MLP就是输出维度为100的全连接层。

- 卷积核的大小是不是越大越好？
  
  不是。
  
  **答：**（1）深度神经网络的精髓是深，而不是宽。我们可以做一个每层很宽（KernelSize大）但层数很少的网络，但是不如每层很窄（KernelSize小）但层数多的网络。
  
  （2）另外，深层的神经网络的感受野实际上就是整张图片的视野。
  
  （3）通常来讲，卷积核的大小就是3x3或者5x5，5x5多用于最浅层，且当图片大小很大的时候。

- 二维卷积，有没有可能同时使用两个不同尺寸的Kernel进行计算，然后计算一个更合适的Kernel，从而提高特征提取的性能？
  
  **答：** GoogleNet就是如此。

- 卷积核如何体现局部性和平移不变性？
  
  **答：**（1）卷积核的大小体现局部性。卷积核没有一次看完全部信息，只看了图像的部分信息。
  
  （2）卷积核本身体现平移不变性。平移不变性是指不管卷积核移动到图像的哪个位置，卷积核本身不变化（每个卷积核识别一种特定的模式）。

- 损失函数抖动比较大的原因？
  
  **答：**（1）数据多样性比较大。每次随机采样的时候看到不一样的东西，这种抖动的存在利索当然。只要最终损失在下降就没问题。
  
  （2）但是，如果光抖动不下降，就需要进一步确定是数据问题还是网络问题。
  
  （3）解决抖动太大的方式是：1.学习率大一点儿 2.批量大小大一点儿。

## 卷积和步幅QA

- 卷积核的大小、填充、步幅的超参数的重要程度？
  
  **答：**（1）一般来说，填充会使得输入和输出的大小是一样的。通常设置为K-1。
  
  （2）通常来讲，步幅为1是最好的。但是，当计算量太大的时候，可以考量将步幅设置为2。特别是处理特别大的图片的时候。
  
  （3）核大小一般是最重要的，一般使用3，最多5。

- 为什么卷积核的边长一般是奇数？
  
  **答：** 一般取奇数。可以取偶数，2和4的卷积核的效果跟3差不多。之所以取奇数，是为了在填充后特征图对称。

- 一般卷积处理完，输出特征图大小都要减半，为什么在某些卷积层又要通过填充的方式保证输入输出特征图大小不变？
  
  **答：** 为了把网络做得更深。如果每次卷积层都将特征图的大小减半，那么只能做很浅的网络。

- 步幅、填充、核大小和通道数这四个超参数，为什么实际中一般不怎么调节？
  
  **答：** 这些参数可以调节。但是，这四个超参数属于网络架构的超参数，在使用经典架构的时候，论文已经指出了这些超参数的值，通常这些值就是局部最优的。

- 针对目前存在大量的经典网络结构的基础上，在实践中应对各种任务时，一般是自己网络架构还是直接套用经典网络结构？
  
  **答：** 可以自己弄，但是试错成本比较高。但是通常来说是直接用经典网络，或在经典网络架构上做一定程度的调整。
  
  另外，网络架构没有想象的那么重要，相对应的是数据的收集和预处理以获得一个高质量是数据集更重要。

- 为什么用3x3的卷积核？3x3的核大小很小。
  
  **答：** 随着神经网络的层数变深，感受野会变得很大。

- 有什么办法可以让超参数也一起训练？
  
  **答：** NAS——有钱人的游戏。

- 若通过多层输入输出形状相同的卷积层，特征会丢失吗？
  
  **答：** 会。从信息论的角度讲，信息永远是会丢失的。我们可以认为，机器学习就是一个很极端的压缩算法。
  
  直观的看，将nMB的图像所蕴含的所有信息压缩到nB的分类标签上，这个分类标签是图片被这个“压缩算法”压缩后得到的语义信息。

- 自动训练参数的话，是不是更容易过拟合？
  
  **答：** 如果验证集设置得好的话，过拟合可以很好的避免。自动训练会带来一定的过拟合，但是可以人为的控制。

- 多层的3x3卷积是否可以等效为层数更少的5x5卷积？
  
  **答：** 效果可以等效。但是3x3卷积计算量更少，计算起来更快。通常在输入图像很大时，网络浅层使用5x5。

- 浅层用大Kernel，深层用小Kernel，相比都用同样大小的Kernel，哪种方案的效果更好？
  
  **答：** 主流做法是，当输入图像特别大的时候，对最浅的那一层，用5x5或者7x7。后面的层继续用3x3，计算量更少，算起来更快。
  
  但是，如果效果并不是那么好。那么都用3x3反而更好，在其他方面的成本更低。

- 多层卷积提取出来的图像特征是不是可以理解为多种不同的纹理特征？
  
  **答：** 可以理解为，一个特定的卷积层就是去看一个特定的纹理特征。

## 多输入和多输出通道QA

- 网络越深，填充越多，是否会影响神经网络的性能？
  
  **答：** 增加了一点点计算性能；不影响模型性能。

- 每个通道的卷积核都不一样吗？同一层不同通道的卷积核大小必须一样吗？
  
  **答：** （1）每个通道的卷积核是不一样的。（2）不同通道的卷积核大小是一样的。为了计算的效率更高。

- 计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？
  
  **答：** （1）bias有一定的作用。但是随着BN层的加入，bias的作用变低。
  
  （2）偏移的作用是：当你的数据均值不为零的时候，偏移会等于均值的负数。
  
  （3）实际上，由于我们会做大量的均一化的操作，所以偏移没有那么大的影响。加偏移或者不加无影响。

- 卷积核的参数怎么来的？
  
  **答：** 核的参数是学出来的。

- 如果是一个RGB图像，加上深度图，相当于输入是4通道的二维卷积吗？做法是和RGB三通道的二维卷积相同吗？
  
  **答：** 不是的。
  
  （1）二维卷积是输入只有高和宽的两个维度的卷积，跟通道没有关系。对于一张RGB图片，虽然他有三个通道，但是对于每个通道，他的都只有高宽两个维度。
  
  （2）加上深度图，相当于加维度而非加通道。即由原本的xy轴空间变为xyz空间，需要用三维卷积来做。
  
  （3）三维卷积，输入大小=[输入通道\*深度\*宽\*高]，这是一个4D的输入，核会变成5D张量，输入还是一个4D的东西。

- 卷积核的数量与输入通道数核输出通道数之间的关系？
  
  **答：** 卷积核数量=输入通道数量\*输出通道数量。即对于输入数据的每一通道，都有输出通道数这么多的卷积核。

- 高频和低频细节的学习有什么讲究么？
  
  **答：** 不用太多注重这个事情。这是信号处理里的讲法，对于图片而言这个概念并不多。通常来说，某些通道会在训练过程中自己去学习高频特征，有的会去学低频。

- 怎么理解1x1卷积不识别空间模式？
  
  **答：** 因为1x1卷积核只计算了其对应的那个中心元素的信息，而没有类似3x3那样计算了中心元素周围的8个元素信息。

- 是否可以用3x3x3和1x1xN的卷积层叠加，来分别进行特征提取和特征融合以及输出通道的调整？
  
  **答：** 是的。MobileNet就是如此。

- 卷积能获取位置信息吗？感觉卷积就是把网络信息提取出来便于函数拟合。
  
  **答：** 能。卷积对位置信息特别敏感，对于输出的（i，j）元素就是输入的（i,j）周围的那一块的信息。

- 多通道时，每个Kernel学习到了不同的参数。那么卷积层中的参数是如何共享的？
  
  **答：** 单一卷积层的不同通道间的卷积核参数不共享。

- 计算复杂度的公式是怎样的？
  
  **答：** 计算复杂度=输入通道数(ci)\*输出通道数(co)\*卷积核高(kh)\*卷积核宽(kw)\*输出张量的高(mh)\*输出张量的宽(mw)

- 什么是feature map?
  
  **答：** 卷积层的输出。

## 池化层QA

- 池化层一般是放在卷积层的前面还是后面？
  
  **答：** 通常放在卷积层的后面。

- 池化层窗口有重叠与没有重叠时影响如何？
  
  **答：** 一般来讲没区别。特别是随着池化层用得越来越少。（1）池化层是为了让卷积层不对位置信息过于敏感.
  
  （2）然而，随着对数据的增强和BN层的引入，池化层的作用几乎不大。

- 池化后计算量是不是减少了？
  
  **答：** 可能减少，可能不减少。
  
  （1）当步幅等于2时，计算量减少了。
  
  （2）池化层步幅为1时，计算量变化不大。

- 为什么现在池化用得越来越少？
  
  **答：** 池化层的作用，（1）让卷积层对位置没有那么敏感；（2）用strides=2让输出特征图的大小减半。
  
  而现在通常用卷积层和strides=2来让输出的特征图大小减半，而不是池化层。
  
  另外，我们会对数据做大量的增强，使得卷积神经网络的稳定性更好。

- 池化层是不是类似做正则？
  
  **答：** 不类似。正则是控制模型的复杂度，池化是让模型更稳健。
